{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sa-y-an/OMR_Net/blob/main/Final_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e69JYmkXRIn"
      },
      "source": [
        "#importing stuffs\n",
        "import numpy as np\n",
        "import gc \n",
        "import os\n",
        "import ctypes\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.layers import LeakyReLU\n",
        "import pandas as pd\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHU5d1O6cJdE"
      },
      "source": [
        "custom activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKJ9yTqhcMmj"
      },
      "source": [
        "# pflu activation function\n",
        "def pflu_activation(x):\n",
        "    return x*0.5*(1+(x/(1+x**2)**0.5))\n",
        "\n",
        "#fpflu actiavtion function\n",
        "def fpflu_activation(x):\n",
        "    return tf.math.maximum(x,x/(1+x**2)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KcU95pTWK8_"
      },
      "source": [
        "Fetching dataset from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdAi67kwVd6C"
      },
      "source": [
        "!wget https://github.com/sa-y-an/answersheet_data/raw/main/folds.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcAg_bZgViiF"
      },
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y1VM_lDVoo4"
      },
      "source": [
        "file_name = \"folds.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    #zip.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIz-OjBXVcH"
      },
      "source": [
        "Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YAL_GZHWVo_"
      },
      "source": [
        "#preparing dataset\n",
        "\n",
        "import pathlib\n",
        "data_dir1 = '/content/folds/1'\n",
        "data_dir2 = '/content/folds/2'\n",
        "data_dir3 = '/content/folds/3'\n",
        "data_dir4 = '/content/folds/4'\n",
        "data_dir5 = '/content/folds/5'\n",
        "\n",
        "data_dir1 = pathlib.Path(data_dir1)\n",
        "data_dir2 = pathlib.Path(data_dir2)\n",
        "data_dir3 = pathlib.Path(data_dir3)\n",
        "data_dir4 = pathlib.Path(data_dir4)\n",
        "data_dir5 = pathlib.Path(data_dir5)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# fetching data from directory\n",
        "\n",
        "img_ds1=tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir1,\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "img_ds2=tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir2,\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "img_ds3=tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir3,\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        " \n",
        "img_ds4=tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir4,\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size)\n",
        "\n",
        "img_ds5=tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        data_dir5,\n",
        "        seed=123,\n",
        "        image_size=(img_height, img_width),\n",
        "        batch_size=batch_size) \n",
        "\n",
        "# normalizing all the images to a float btw 0 , 2555\n",
        "\n",
        "def process(image,label):\n",
        "    image = tf.cast(image/255. ,tf.float32)\n",
        "    return image,label\n",
        "\n",
        "img_ds1 = img_ds1.map(process)\n",
        "img_ds2 = img_ds2.map(process)\n",
        "img_ds3 = img_ds3.map(process)\n",
        "img_ds4 = img_ds4.map(process)\n",
        "img_ds5 = img_ds5.map(process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMmeQhmiY9ep"
      },
      "source": [
        "Uncomment that model which you will use to save memory space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0PaK22vXqWh"
      },
      "source": [
        "# transfer learning model \n",
        "# mobile net V2 model used\n",
        "\n",
        "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "\n",
        "pretrained_model_without_top_layer = hub.KerasLayer(\n",
        "    feature_extractor_model, input_shape=(224, 224, 3), trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Buvyd5GYaE0_"
      },
      "source": [
        "classification_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
        "\n",
        "pretrained_model_without_top_layer_class = hub.KerasLayer(\n",
        "    classification_model, input_shape=(224, 224, 3), trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq8QcaunXv4B"
      },
      "source": [
        "# Dense Net 201 Model\n",
        "\n",
        "# base_model_dn201 = tf.keras.applications.DenseNet201(\n",
        "#     include_top=True,\n",
        "#     weights=\"imagenet\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRbpG1NjXy_p"
      },
      "source": [
        "# VGG19 Model\n",
        "\n",
        "# base_model_vgg19 = tf.keras.applications.VGG19(\n",
        "#     include_top=True,\n",
        "#     weights=\"imagenet\"\n",
        "# )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb4wSql0X4Q-"
      },
      "source": [
        "# Functions \n",
        "\n",
        "# chosing the correct fold\n",
        "def create_dataset(i) :    \n",
        "    \n",
        "    if i == 0 :\n",
        "        train_dsu = img_ds2.concatenate(img_ds3.concatenate(img_ds4.concatenate(img_ds5)))\n",
        "        test_dsu = img_ds1\n",
        "    if i == 1 :\n",
        "        train_dsu = img_ds1.concatenate(img_ds3.concatenate(img_ds4.concatenate(img_ds5)))\n",
        "        test_dsu = img_ds2\n",
        "    if i == 2 :\n",
        "        train_dsu = img_ds1.concatenate(img_ds2.concatenate(img_ds4.concatenate(img_ds5)))\n",
        "        test_dsu = img_ds3\n",
        "    if i == 3 :\n",
        "        train_dsu = img_ds1.concatenate(img_ds2.concatenate(img_ds3.concatenate(img_ds5)))\n",
        "        test_dsu = img_ds4\n",
        "    if i == 4 :\n",
        "        train_dsu = img_ds1.concatenate(img_ds2.concatenate(img_ds3.concatenate(img_ds4)))\n",
        "        test_dsu = img_ds5  \n",
        "        \n",
        "    return (train_dsu,test_dsu)\n",
        "\n",
        "\n",
        "# tuning and caching the data for faster training\n",
        "def atune(train_dsu,test_dsu):\n",
        "    \n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    train_dsu = train_dsu.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    test_dsu = test_dsu.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "    \n",
        "    return (train_dsu,test_dsu)\n",
        "\n",
        "\n",
        "\n",
        "# division  of the folds into train test\n",
        "def train_test_val(train_dsu):\n",
        "\n",
        "    DATASET_SIZE = tf.data.experimental.cardinality(train_dsu).numpy()\n",
        "    train_size = int(0.8 * DATASET_SIZE)\n",
        "    val_size = int(0.2 * DATASET_SIZE)\n",
        "\n",
        "    train_dsu = train_dsu.shuffle(DATASET_SIZE)\n",
        "    train_dataset = train_dsu.take(train_size)\n",
        "    val_dataset = train_dsu.skip(train_size)\n",
        "    \n",
        "    return (train_dataset,val_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd4ubsCgZ7ef"
      },
      "source": [
        "def create_model_mobnetV2_class() :\n",
        "    \n",
        "    num_classes = 3\n",
        "    \n",
        "    omr_model = tf.keras.Sequential([\n",
        "      pretrained_model_without_top_layer_class,\n",
        "      layers.Dense(640, activation= 'relu'),   \n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(320,activation= fpflu_activation ),\n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(160,activation= pflu_activation ),\n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(num_classes,activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)\n",
        "    \n",
        "    \n",
        "    omr_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "#     print(omr_model.summary())\n",
        "    \n",
        "    return omr_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX3B8WJ4YFoq"
      },
      "source": [
        "# tf learning on mobnet V2 Model\n",
        "def create_model_mobnetV2() :\n",
        "    \n",
        "    num_classes = 3\n",
        "    \n",
        "    omr_model = tf.keras.Sequential([\n",
        "      pretrained_model_without_top_layer,\n",
        "      layers.Dense(640, activation= 'relu'),   \n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(320,activation= fpflu_activation ),\n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(160,activation= pflu_activation ),\n",
        "      layers.Dropout(0.25),\n",
        "      layers.Dense(num_classes,activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)\n",
        "    \n",
        "    \n",
        "    omr_model.compile(optimizer='RMSprop',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "#     print(omr_model.summary())\n",
        "    \n",
        "    return omr_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ybBsgVYI44"
      },
      "source": [
        "# tf learning on densenet 201\n",
        "def create_dense_net() :\n",
        "\n",
        "\n",
        "    num_classes = 3\n",
        "\n",
        "\n",
        "    # Freeze base model\n",
        "    base_model_dn201.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "    omr_model = tf.keras.Sequential([\n",
        "      base_model_dn201,\n",
        "      layers.Dense(640, activation= 'relu'),   \n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(320,activation= fpflu_activation ),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(160,activation= pflu_activation ),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(num_classes,activation='softmax'),\n",
        "    ])\n",
        "\n",
        "\n",
        "    omr_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    # print(omr_model.summary())\n",
        "    \n",
        "    return omr_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVBTE5pcYLxL"
      },
      "source": [
        "# tf learning on densenet 201\n",
        "def create_vgg19() :\n",
        "\n",
        "\n",
        "    num_classes = 3\n",
        "\n",
        "\n",
        "    # Freeze base model\n",
        "    base_model_vgg19.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "    omr_model = tf.keras.Sequential([\n",
        "      base_model_vgg19,\n",
        "      layers.Dense(640, activation= 'relu'),   \n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(320,activation= fpflu_activation ),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(160,activation= pflu_activation ),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(num_classes,activation='softmax'),\n",
        "    ])\n",
        "\n",
        "\n",
        "    omr_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    # print(omr_model.summary())\n",
        "    \n",
        "    return omr_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHOI6Ymf0k8G"
      },
      "source": [
        "all_models = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BBKjNrmYPGf"
      },
      "source": [
        "# function used for iteratively training the model on folds\n",
        "\n",
        "def model_training(epochs) :\n",
        "    \n",
        "    # vald_acc and test_d acc list\n",
        "    vald_acc = []\n",
        "    testd_acc = []\n",
        "\n",
        "    \n",
        "    for fold in range (5):\n",
        "\n",
        "        train_dsu , test_dsu = create_dataset(fold)\n",
        "        train_dsu , test_dsu = atune(train_dsu,test_dsu)\n",
        "        train_dataset,val_dataset = train_test_val(train_dsu)\n",
        "\n",
        "        del train_dsu\n",
        "\n",
        "\n",
        "\n",
        "        print( str('This is for fold - ') + str(fold+1))\n",
        "\n",
        "        \n",
        "        #set the model type here\n",
        "        model = create_model_mobnetV2()\n",
        "\n",
        "        tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(fold), histogram_freq=1)\n",
        "\n",
        "\n",
        "        hv = model.fit(train_dataset,validation_data=val_dataset,epochs=epochs ,callbacks = [tb_callback] )\n",
        "        vald_acc.append(hv)\n",
        "        \n",
        "        print('\\n')\n",
        "\n",
        "        gc.collect() # ggarbage collection\n",
        "\n",
        "        ht = model.evaluate(test_dsu)       \n",
        "        testd_acc.append(ht)\n",
        "\n",
        "        # appending all models for future reference\n",
        "        all_models.append(model)\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "        print('\\n')\n",
        "\n",
        "        del test_dsu, train_dataset,val_dataset, model , ht , hv # deleting variables to clear memory\n",
        "        \n",
        "        libc = ctypes.CDLL(\"libc.so.6\") # clearing cache \n",
        "        libc.malloc_trim(0)\n",
        "\n",
        "        gc.collect() # garbage collection\n",
        "        \n",
        "        \n",
        "    return vald_acc,testd_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPG41IiyYTI0"
      },
      "source": [
        "# set the number of epochs \n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5v5xApHYVR-"
      },
      "source": [
        "vald_acc,testd_acc = model_training(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha1ey7UjYj5B"
      },
      "source": [
        "l = []\n",
        "for i in testd_acc :\n",
        "    l.append(i[1])\n",
        "\n",
        "    \n",
        "print(l)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLP9hfy7Y0Y3"
      },
      "source": [
        "avg_val_acc = 0\n",
        "for i in range(5) :\n",
        "    avg_val_acc +=  vald_acc[i].history['val_accuracy'][epochs-1]\n",
        "    \n",
        "print(avg_val_acc/5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4bEnzwVY1Md"
      },
      "source": [
        "d = {'fold1' : [l[0]], 'fold2' : [l[1]] , 'fold3' : [l[2]] , 'fold4' : [l[3]] , 'fold5' : [l[4]] , 'fold_avg' : [np.mean(testd_acc, axis = 0)[1]] , 'vald_acc' : [avg_val_acc/5]}\n",
        "df = pd.DataFrame(data=d)\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz0Z_HC0Y3o_"
      },
      "source": [
        "np.mean(testd_acc, axis = 0)[1]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC2MW-HYoT0c"
      },
      "source": [
        "Loading Tensorboard for analysis of results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbe8ymYp0_YR"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMsDCx73txN"
      },
      "source": [
        "Downloading Models that give good accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qda-qLyt7BCx"
      },
      "source": [
        "model2 = all_models[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oRSvgx37Q1W"
      },
      "source": [
        "model2.save('fold2_95_88.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8Kb4smd3swV"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('fold2_95_88.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}